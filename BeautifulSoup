原创url：http://www.jianshu.com/p/ef2f246cae46
参考url：http://blog.csdn.net/abclixu123/article/details/38502993
参考url： http://cuiqingcai.com/1319.html

在进入正题前先说一下每次完成代码后，可以用ctrl+alt+l对代码进行自动格式规范化。

在爬取网页中有用的信息时，通常是对存在于网页中的文本或各种不同标签的属性值进行查找，Beautiful Soup中内置了一些查找方式，最常用的是find()和find_all()函数。[文献引自http://blog.csdn.net/abclixu123/article/details/38502993 ]。 同时通过soup.find_all()得到的所有符合条件的结果和soup.select()一样都是列表list，而soup.find()只返回第一个符合条件的结果，所以soup.find()后面可以直接接.text或者get_text()来获得标签中的文本。

一、find()用法
find(name,attrs,recursive,text,**wargs)
这些参数相当于过滤器一样可以进行筛选处理，不同的参数过滤可以应用到以下情况：
查找标签，基于name参数
查找文本，基于text参数
基于正则表达式的查找
查找标签的属性，以及基于attrs参数
基于函数的查找

<ul id="producers">
        <li class="producerlist">
            <div class="name">plants</div>
            <div class="number">100000</div>
        </li>
        <li class="producerlist">
            <div class="name">algae</div>
            <div class="number">100000</div>
        </li>
</ul>
以上面的例子来看：
(1)ul,li,div这些就是标签；

用法p=soup.find('ul') ，那么返回结果是第一个ul标签以及<xx>...</xx>的所有内容，即上面的代码；注意若用p=soup.find('ul').get_text()那么结果不是...的所有内 容，而应该是plants 10000 algae 10000，即...中的标签不算text文本。
(2)<xx>...</xx>之间的内容就是文本；
基于文本内容的查找也可以用soup.find()，但必须用到参数text，

用法p=soup.find(text='algae')，print(p)得到的结果就是algae
(3)正则表达式后面自己另外去学习；

(4)ul id="producers">中的id即标签属性，那么我们可以查找具有特定标签的属性；

用法p=soup.find('ul', id="producers")，那么可以得到<xx>...</xx>的所有结果，其特点是把标签更一步精确化以便于查找。
对于大多数的情况可以用上面的方法解决，但是有两种情况则要用到参数attrs:一是标签字符中带有-，比如data-custom;二是class不能看作标签属性。解决的办法是在attrs属性用字典进行传递参数：
soup.find(attrs={'data-custom':'xxx'})以及 soup.find(attrs={'class':'xxx'})
(5)基于函数的查找也暂时搁置。

二、find_all()用法
应用到find()中的不同过滤参数同理可以用到find_all()中，相比find()，find_all()有个额外的参数limit，如下所示：
p=soup.find_all(text='algae',limit=2)
实际上find()也就是当limit=1时的find_all()。

关于find和find_all的用法先学习这么多，如果后面有涉及到更深入再去研究。

到今天基本把赶集网北京地区的所有内容爬了一遍，但其中涉及到的使用代理ip时还是会报错，等这周日听课时来解决。马上就要用爬取的内容进行统计分析了，所以下一篇会学习非关系型数据库mongodb的知识。

作者：是蓝先生
链接：http://www.jianshu.com/p/ef2f246cae46
來源：简书




Python之爬取网页时遇到的问题——BeautifulSoup
     记下两个与本文内容不太相关的知识点。

     import re   对正则表达式支持的包。

     str(soup.p).decode('utf-8')     对标签内容转码。



     Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树。 它提供简单又常用的导航，搜索以及修改剖析树的操作。它可以大大节省你的编程时间。



     通俗的来说，就是在

            req = urllib2.Request(url, headers=headers)
            page = urllib2.urlopen(req, timeout=60)
            contents = page.read()

    之后，对contents进行解析  soup = BeautifulSoup(contents, 'html.parser')，这样构建的是Python标准库，然后我们便可以对这个soup对象进行一系列的操作，提取我们所需要的元素。

    我们对用法进行一下解释：

    soup.title    得到的是<title>标签，以及标签里的内容。但是得到的是所有<title>标签中的第一个。<title>这里是内容</title>，红字部分。

    soup.title.name    得到的是<title>标签的name属性的值。得到的是所有<title>标签中的第一个。

    soup.title.string    得到的是<title>开始和结束标签之间的值。<title>这里是内容</title>，即红字部分。得到的是所有<title>标签中的第一个。

    soup.find_all('title')     得到所有标签为<title>的的标签，以及标签里的内容。返回的是一个序列，可以对它循环，得到自己想要的东西。

    soup.find(id='3')     得到id为3的标签。

    soup.p.get_text()    返回的是<p>标签的文本。

    soup.a['href']          返回<a>标签的 herf 值 。

    soup.head.contents      获得head下的所有子孩子。以列表的形式返回结果，可以使用 [num]  的形式获得 。获得标签，使用.name 就可以。

    print soup.find_all(text="Elsie")     获得文本为Elsie的标签。

    print soup.find_all("a", limit=2)       返回两个<a>标签。

    string属性，如果超过一个标签的话，那么就会返回None，否则就返回第一个标签的string。超过一个标签的话，可以试用strings。

    获取标签的孩子，可以使用children，但是不能print soup.head.children，没有返回列表，返回的是 <listiterator object at 0x108e6d150>,不过使用list可以将其转化为列表。可以使用for 语句遍历里面的孩子。

     向上查找可以用parent函数，如果查找所有的，那么可以使用parents函数。

     查找下一个兄弟使用next_sibling，查找上一个兄弟节点使用previous_sibling,如果是查找所有的，那么在对应的函数后面加s就可以。



     soup.select()找到的数据，返回为list类型，即，寻找到的是所有符合要求的数据。



      soup.select('div')     直接返回所有div标签的所有内容

      soup.select('.ebox')      . 这个点表示查询class="ebox"的，所有标签内容。

      len(soup.select('.ebox'))      可以查询出20条数据。

      soup.select('#index_nav')      查找所有id为 index_nav 的标签。

      soup.select('div #index_nav')    表示寻找div标签中id为index_nav的标签内容。

      soup.select('p[class="etitle"]')     查找所有class为etitle的<p>标签。





http://cuiqingcai.com/1319.html    ，    http://blog.csdn.net/akak714/article/details/50130743   这两个网址可以学习一下。