原创url：http://www.jianshu.com/p/ef2f246cae46
参考url：http://blog.csdn.net/abclixu123/article/details/38502993
参考url： http://cuiqingcai.com/1319.html

在做一个爬虫的时候，有类似上面的很多链接，我用BeautifulSoup怎样得到里边的href和title呢？
<a class="name" href="http://www.renren.com/profile.do?id=000000000" target="_blank" title="XXX">XXX</a>
<a class="name" href="http://www.renren.com/profile.do?id=111111111" target="_blank" title="YYY">YYY</a>
<a class="name" href="http://www.renren.com/profile.do?id=222222222" target="_blank" title="ZZZ">ZZZ</a>


f=open("aaaaaa.txt", "wb")
for url in soup.find_all("a", attrs={"class": "name"}):
 print url
 f.write(url.get('href'))
 f.write(' ')
 f.write(url.get('title'))
 f.write('\r\n')
f.close()


这样简单的就能得到我想要的里边的href和title了


#############################################################################################################################
在进入正题前先说一下每次完成代码后，可以用ctrl+alt+l对代码进行自动格式规范化。

在爬取网页中有用的信息时，通常是对存在于网页中的文本或各种不同标签的属性值进行查找，Beautiful Soup中内置了一些查找方式，最常用的是find()和find_all()函数。[文献引自http://blog.csdn.net/abclixu123/article/details/38502993 ]。 同时通过soup.find_all()得到的所有符合条件的结果和soup.select()一样都是列表list，而soup.find()只返回第一个符合条件的结果，所以soup.find()后面可以直接接.text或者get_text()来获得标签中的文本。

一、find()用法
find(name,attrs,recursive,text,**wargs)
这些参数相当于过滤器一样可以进行筛选处理，不同的参数过滤可以应用到以下情况：
查找标签，基于name参数
查找文本，基于text参数
基于正则表达式的查找
查找标签的属性，以及基于attrs参数
基于函数的查找

<ul id="producers">
        <li class="producerlist">
            <div class="name">plants</div>
            <div class="number">100000</div>
        </li>
        <li class="producerlist">
            <div class="name">algae</div>
            <div class="number">100000</div>
        </li>
</ul>
以上面的例子来看：
(1)ul,li,div这些就是标签；

用法p=soup.find('ul') ，那么返回结果是第一个ul标签以及<xx>...</xx>的所有内容，即上面的代码；注意若用p=soup.find('ul').get_text()那么结果不是...的所有内 容，而应该是plants 10000 algae 10000，即...中的标签不算text文本。
(2)<xx>...</xx>之间的内容就是文本；
基于文本内容的查找也可以用soup.find()，但必须用到参数text，

用法p=soup.find(text='algae')，print(p)得到的结果就是algae
(3)正则表达式后面自己另外去学习；

(4)ul id="producers">中的id即标签属性，那么我们可以查找具有特定标签的属性；

用法p=soup.find('ul', id="producers")，那么可以得到<xx>...</xx>的所有结果，其特点是把标签更一步精确化以便于查找。
对于大多数的情况可以用上面的方法解决，但是有两种情况则要用到参数attrs:一是标签字符中带有-，比如data-custom;二是class不能看作标签属性。解决的办法是在attrs属性用字典进行传递参数：
soup.find(attrs={'data-custom':'xxx'})以及 soup.find(attrs={'class':'xxx'})
(5)基于函数的查找也暂时搁置。

二、find_all()用法
应用到find()中的不同过滤参数同理可以用到find_all()中，相比find()，find_all()有个额外的参数limit，如下所示：
p=soup.find_all(text='algae',limit=2)
实际上find()也就是当limit=1时的find_all()。

关于find和find_all的用法先学习这么多，如果后面有涉及到更深入再去研究。

到今天基本把赶集网北京地区的所有内容爬了一遍，但其中涉及到的使用代理ip时还是会报错，等这周日听课时来解决。马上就要用爬取的内容进行统计分析了，所以下一篇会学习非关系型数据库mongodb的知识。

作者：是蓝先生
链接：http://www.jianshu.com/p/ef2f246cae46
來源：简书




Python之爬取网页时遇到的问题——BeautifulSoup
     记下两个与本文内容不太相关的知识点。

     import re   对正则表达式支持的包。

     str(soup.p).decode('utf-8')     对标签内容转码。



     Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树。 它提供简单又常用的导航，搜索以及修改剖析树的操作。它可以大大节省你的编程时间。



     通俗的来说，就是在

            req = urllib2.Request(url, headers=headers)
            page = urllib2.urlopen(req, timeout=60)
            contents = page.read()

    之后，对contents进行解析  soup = BeautifulSoup(contents, 'html.parser')，这样构建的是Python标准库，然后我们便可以对这个soup对象进行一系列的操作，提取我们所需要的元素。

    我们对用法进行一下解释：

    soup.title    得到的是<title>标签，以及标签里的内容。但是得到的是所有<title>标签中的第一个。<title>这里是内容</title>，红字部分。

    soup.title.name    得到的是<title>标签的name属性的值。得到的是所有<title>标签中的第一个。

    soup.title.string    得到的是<title>开始和结束标签之间的值。<title>这里是内容</title>，即红字部分。得到的是所有<title>标签中的第一个。

    soup.find_all('title')     得到所有标签为<title>的的标签，以及标签里的内容。返回的是一个序列，可以对它循环，得到自己想要的东西。

    soup.find(id='3')     得到id为3的标签。

    soup.p.get_text()    返回的是<p>标签的文本。

    soup.a['href']          返回<a>标签的 herf 值 。

    soup.head.contents      获得head下的所有子孩子。以列表的形式返回结果，可以使用 [num]  的形式获得 。获得标签，使用.name 就可以。

    print soup.find_all(text="Elsie")     获得文本为Elsie的标签。

    print soup.find_all("a", limit=2)       返回两个<a>标签。

    string属性，如果超过一个标签的话，那么就会返回None，否则就返回第一个标签的string。超过一个标签的话，可以试用strings。

    获取标签的孩子，可以使用children，但是不能print soup.head.children，没有返回列表，返回的是 <listiterator object at 0x108e6d150>,不过使用list可以将其转化为列表。可以使用for 语句遍历里面的孩子。

     向上查找可以用parent函数，如果查找所有的，那么可以使用parents函数。

     查找下一个兄弟使用next_sibling，查找上一个兄弟节点使用previous_sibling,如果是查找所有的，那么在对应的函数后面加s就可以。



     soup.select()找到的数据，返回为list类型，即，寻找到的是所有符合要求的数据。



      soup.select('div')     直接返回所有div标签的所有内容

      soup.select('.ebox')      . 这个点表示查询class="ebox"的，所有标签内容。

      len(soup.select('.ebox'))      可以查询出20条数据。

      soup.select('#index_nav')      查找所有id为 index_nav 的标签。

      soup.select('div #index_nav')    表示寻找div标签中id为index_nav的标签内容。

      soup.select('p[class="etitle"]')     查找所有class为etitle的<p>标签。





http://cuiqingcai.com/1319.html    ，    http://blog.csdn.net/akak714/article/details/50130743   这两个网址可以学习一下。

几个参考练习网址：http://www.cnblogs.com/cwmizlp/p/7018764.html
                 http://blog.csdn.net/u011974639/article/details/72869487#使用bs4快速定位标签
                 https://zhuanlan.zhihu.com/p/21377121
                 https://yq.aliyun.com/articles/26026
                 http://www.cocoachina.com/programmer/20150227/11198.html
                 https://www.zhihu.com/question/48408140
                 http://blog.csdn.net/u013007900/article/details/54728408
                 http://www.cnblogs.com/vamei/archive/2013/03/12/2954938.html     #以上都为BautifulSoup查阅网址
                 http://www.jb51.net/article/92821.htm   #格式化字符串
                 http://www.jb51.net/article/63672.htm
                 http://www.jb51.net/article/67161.htm   python踩坑实录



 #############################################################################################################################
 Beautiful Soup使用时，一般可以通过指定对应的name和attrs去搜索，特定的名字和属性，以找到所需要的部分的html代码。
但是，有时候，会遇到，对于要处理的内容中，其name或attr的值，有多种可能，尤其是符合某一规律，此时，就无法写成固定的值了。
所以，就可以借助正则表达式来解决此问题。
比如，

<div class="icon_col">
    <h1 class="h1user">crifan</h1>
</div>
对应的BeautifulSoup代码如下：

h1userSoup = soup.find(name="h1", attrs={"class":"h1user"});
而如果html是这种：

<div class="icon_col">
    <h1 class="h1user">crifan</h1>
    <h1 class="h1user test1">crifan 123</h1>
    <h1 class="h1user test2">crifan 456</h1>
</div>
那么想要一次性地找到所有的，符合条件的h1的部分的代码，则之前的写法，就只能找到单个的class="h1user"的部分，剩下的两个

class="h1user test1"
class="h1user test2"
就找不到了。
那么，此时，就可以用到，BeautifulSoup中非常好用的，非常强大的功能：
attrs中支持正则表达式的写法
了。
就可以写成：
h1userSoupList = soup.findAll(name="h1", attrs={"class":re.compile(r"h1user(\s\w+)?")});
就可以一次性地，找到：

class="h1user"

class="h1user test1"

class="h1user test2"
了。

<div aria-lable="xxx">
之类的标签，xxx的内容未知（可变）的前提下
想要查找到对应的此div标签，之前不知道如何实现。
如果写成：
sopu.findAll("div", attrs={"aria-lable": "xxx"});
则xxx必须写出来，如果不写出来属性值，也就没法用上attrs了，就没法实现此处查找特性属性值的标签了。
所以针对：

<div aria-label="5星, 747 份评分" class="rating" role="img" tabindex="-1">
 <div>
 <span class="rating-star">
 </span>
 <span class="rating-star">
 </span>
 <span class="rating-star">
 </span>
 <span class="rating-star">
 </span>
 <span class="rating-star">
 </span>
 </div>
 <span class="rating-count">
 747 份评分
 </span>
</div>
可以通过：

soup.findAll("div", attrs={"aria-lable": True});
去查找到属性包含aria-lable的div标签的。
所以，对于上面的，之前不知道如何处理：
用BeautifulSoup查找未知属性值，但是已知属性的名字的标签
则此处，就可以针对：

<div aria-lable="xxx">
去用：

sopu.findAll("div", attrs={"aria-lable": True});
就可以查找到对应的包含属性aria-lable的div标签了。


##############################################################################################################################
IP和端口 tr.td 标签里面，tr有class属性，属性有两种情况的值，
对于这点我们可以用正则表达式来匹配下。当提取某一个标签里的具体内容时，
可以用bs的 .string属性，注意：用 .string 属性来提取标签里的内容时，
该标签应该是只有单个节点的。比如上面的 td 标签那样。下面直接上代码了

#匹配带有class属性的tr标签
taglist = soup.find_all('tr', attrs={'class': re.compile("(odd)|()")})
for trtag in taglist:
    tdlist = trtag.find_all('td')  #在每个tr标签下,查找所有的td标签
    print tdlist[1].string   #这里提取IP值
    print tdlist[2].string   #这里提取端口值